{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsJICM-vveWW",
        "outputId": "e238d8b3-9646-499d-eb0a-e79e3bf4ea27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79b980b62830>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivB3NdV-y3RH",
        "outputId": "78288c66-62a1-4126-930c-4fd914027e5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-23 19:39:10--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  3.96MB/s    in 0.3s    \n",
            "\n",
            "2025-07-23 19:39:11 (3.96 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the experts of the NN"
      ],
      "metadata": {
        "id": "FaGEe15Rzyzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eppert mocule\n",
        "class Expert(nn.Module):\n",
        "\n",
        "  \"\"\"An MLP is a simple linear layer folowed by an non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "PRuq1IkLzAC6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding how gating works\n",
        "num_experts = 3\n",
        "top_k = 2\n",
        "n_embd = 32\n",
        "\n",
        "# Example multi-head attention output for simple illustrative example.\n",
        "mh_output = torch.randn(1, 4, n_embd)\n",
        "\n",
        "topkgate_linear = nn.Linear(n_embd, num_experts)\n",
        "logits = topkgate_linear(mh_output)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q89N1oeb4UCh",
        "outputId": "ab679446-5655-49b6-80b2-095e429d08a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.2753, -1.8810, -0.1553],\n",
            "         [-0.1253, -0.5594, -0.2212],\n",
            "         [-0.3208, -0.1205, -0.0061],\n",
            "         [-0.3639, -0.2565,  0.1207]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing load balancing"
      ],
      "metadata": {
        "id": "ppq1T_Bh7TDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We start with topk load balancing\n",
        "top_k_logits, top_k_indicies = logits.topk(top_k, dim=-1)\n",
        "top_k_logits, top_k_indicies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6xDlCvc44LG",
        "outputId": "75ea7e65-ba8d-4e73-e93d-68f77ae739d0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.2753, -0.1553],\n",
              "          [-0.1253, -0.2212],\n",
              "          [-0.0061, -0.1205],\n",
              "          [ 0.1207, -0.2565]]], grad_fn=<TopkBackward0>),\n",
              " tensor([[[0, 2],\n",
              "          [0, 2],\n",
              "          [2, 1],\n",
              "          [2, 1]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply -ing softmax"
      ],
      "metadata": {
        "id": "uxQmoxos9Zz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = torch.full_like(logits, float(\"-inf\"))\n",
        "sparse_logits = zeros.scatter(-1, top_k_indicies, top_k_logits)\n",
        "sparse_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp2-UMfd7qKK",
        "outputId": "c825338c-a9dc-4afc-96be-48713fab1a84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2753,    -inf, -0.1553],\n",
              "         [-0.1253,    -inf, -0.2212],\n",
              "         [   -inf, -0.1205, -0.0061],\n",
              "         [   -inf, -0.2565,  0.1207]]], grad_fn=<ScatterBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gating_output = F.softmax(sparse_logits, dim=-1)\n",
        "gating_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_44j5EI9kDW",
        "outputId": "73af609e-d587-437c-f598-dea5d783e18e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.6060, 0.0000, 0.3940],\n",
              "         [0.5239, 0.0000, 0.4761],\n",
              "         [0.0000, 0.4714, 0.5286],\n",
              "         [0.0000, 0.4068, 0.5932]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a class for TopKRouting"
      ],
      "metadata": {
        "id": "4LZ7laSc_iGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First define the top k router module\n",
        "class TopKRouter(nn.Module):\n",
        "  def __init__(self, n_embd, num_experts, top_k):\n",
        "    super(TopKRouter, self).__init__()\n",
        "    self.top_k = top_k\n",
        "    self.linear = nn.Linear(n_embd, num_experts)\n",
        "\n",
        "  def forward(self, mh_output):\n",
        "    # mh_output is the output tensor from multihead self attention block\n",
        "    logits = self.linear(mh_output)\n",
        "    top_k_logits, indicies = logits.topk(self.top_k, dim=-1)\n",
        "    zeros = torch.full_like(logits, float('-inf'))\n",
        "    sparse_logits = zeros.scatter(-1, indicies, top_k_logits)\n",
        "    router_output = F.softmax(sparse_logits, dim=-1)\n",
        "    return router_output, indicies"
      ],
      "metadata": {
        "id": "CmJSeHZz-WH8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chanign the above to accomadate noise top-k gating\n",
        "class NoisyTopkTouter(nn.Module):\n",
        "  def __init__(self, n_embd, num_experts, top_k):\n",
        "    super(NoisyTopkTouter, self).__init__()\n",
        "    self.top_k = top_k\n",
        "    # layer for router logits\n",
        "    self.topkroute_linear = nn.Linear(n_embd, num_experts)\n",
        "    self.noise_linear  = nn.Linear(n_embd, num_experts)\n",
        "\n",
        "  def forward(self, mh_output):\n",
        "    # mh_output is the output tensor from multihead self attention block\n",
        "    logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "    # Noise logits\n",
        "    noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "    # Adding scaled unit gaussian noise to the logits\n",
        "    noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
        "    noisy_logits = logits + noise\n",
        "\n",
        "    top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "    zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "    sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "    router_output = F.softmax(sparse_logits, dim=-1)\n",
        "    return router_output, indices"
      ],
      "metadata": {
        "id": "GZPkSOsWF46-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a spares MoE model"
      ],
      "metadata": {
        "id": "YNqqzN1eMKD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseMoE(nn.Module):\n",
        "  def __init__(self, n_embd, num_experts, top_k):\n",
        "    super(SparseMoE, self).__init__()\n",
        "    self.router = NoisyTopkTouter(n_embd, num_experts, top_k)\n",
        "    self.experts = nn.ModuleList([Expert(n_embd) for _ in range(num_experts)])\n",
        "    self.top_k = top_k\n",
        "\n",
        "  def forward(self, x):\n",
        "    gating_output, indices = self.router(x)\n",
        "    final_output = torch.zeros_like(x)\n",
        "\n",
        "    # Reshape inputs for batch processing\n",
        "    flat_x = x.view(-1, x.size(-1))\n",
        "    flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "    # Process each expert in parallel\n",
        "    for i, expert in enumerate (self.experts):\n",
        "      # Create a mask for the inputs where the current expert is in top-k\n",
        "      expert_mask = (indices == i).any(dim=-1)\n",
        "      flat_mask = expert_mask.view(-1)\n",
        "\n",
        "      if flat_mask.any():\n",
        "        expert_input = flat_x[flat_mask]\n",
        "        expert_output = expert(expert_input)\n",
        "\n",
        "        # Extract and apply gating scores\n",
        "        gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "        weighted_output = expert_output * gating_scores\n",
        "\n",
        "        # Update final output additively by indexing and adding\n",
        "        final_output[expert_mask] += weighted_output.squeeze(1)\n",
        "\n",
        "    return final_output\n",
        "\n"
      ],
      "metadata": {
        "id": "OJUKlV-JMJgy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code the entire transformer block: Part 1 (Multi-head Attention)"
      ],
      "metadata": {
        "id": "7NPHWwF1Vbrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "  \"\"\"one head of self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    q = self.query(x)\n",
        "\n",
        "    #compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei,dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    #perform the weighted aggregation of the values\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" We run multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "  def __init__(self, n_head, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "frO8Ha6TVa6A"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code the entire transformer block"
      ],
      "metadata": {
        "id": "OI3X7EkxYAMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # First create a self attnetnio + mixture of experts block, that may be repeated several times\n",
        " # Xopy pasting key architecture variables for clarity\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head, num_experts, top_k):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.smoe = SparseMoE(n_embd, num_experts, top_k)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.smoe(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "jCV5IKHDX_Kh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defnine the language model architecture"
      ],
      "metadata": {
        "id": "uGLdt6FMaX50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally we put it all together\n",
        "class SparseMoELanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads of the logits for the next token from a lookup table\n",
        "    self.t_embd = nn.Embedding(vocab_size, n_embd)\n",
        "    self.p_embd = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, num_experts=num_experts, top_k=top_k) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    tok_emb = self.t_embd(idx)\n",
        "    pos_emb = self.p_embd(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, X = logits.shape\n",
        "      logits = logits.view(B*T, X)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      #focus only on the last time step\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to get the probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=-1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "AGUIOUeqaXUL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create training and testing data"
      ],
      "metadata": {
        "id": "XNrLbPUBs34b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1137)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in teh test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "#creat mapping from chars to integers\n",
        "stoi = { ch:i for i, ch in enumerate(chars)}\n",
        "itos = { i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# trian and test sets\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#data loading\n",
        "def get_batch(split):\n",
        "  # generate a small batcho f data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "tYNtfpbvafRL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define LLM Loss"
      ],
      "metadata": {
        "id": "NUZvUcKO6owX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "RSnCxu796eot"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define training loop params"
      ],
      "metadata": {
        "id": "0U-ftQa38DHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 200\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_interval = 100\n",
        "eval_iters = 400\n",
        "head_size = 16\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "num_experts = 8\n",
        "top_k = 2"
      ],
      "metadata": {
        "id": "Wq0CT9fm8BSo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializ the model"
      ],
      "metadata": {
        "id": "o-NX2SZy_32p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kaiming_init_weights(m):\n",
        "  if isinstance (m, (nn.Linear)):\n",
        "    init.kaiming_normal_(m.weight)"
      ],
      "metadata": {
        "id": "Kb5kEknr_RvI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SparseMoELanguageModel()\n",
        "model.apply(kaiming_init_weights)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAW0oyNVATZd",
        "outputId": "5be067b7-b2c2-41e9-e1c5-c704de994ee1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseMoELanguageModel(\n",
              "  (t_embd): Embedding(65, 128)\n",
              "  (p_embd): Embedding(32, 128)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkTouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkTouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkTouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkTouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkTouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkTouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (6): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkTouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (7): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkTouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the pretrained model"
      ],
      "metadata": {
        "id": "5L4g5U5zCJv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = model.to(device)\n",
        "\n",
        "# display the params in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_iters == 0 or iter == max_iters - 1:\n",
        "\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6sMA8BLCJd3",
        "outputId": "adae3356-6237-4f9e-d667-b386b160c759"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.996545 M parameters\n",
            "step 0: train loss 5.3314, val loss 5.3018\n",
            "step 199: train loss 2.5056, val loss 2.4987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference - Generate from the model"
      ],
      "metadata": {
        "id": "kbif9r1PM0T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBPq3OgyAXi1",
        "outputId": "efff0de7-94e2-4f0e-f869-b8aed7c78886"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Draner hak.\n",
            "Wh nd mithe\n",
            "ARI ty iroo n us at mandwith fat ncor me no ous' helo Pyongote!\n",
            "Buderivet w and: y re sier d,e.\n",
            "e, blas, ODorofe bewie d:\n",
            "Ane be ge so hovend be itourtou\n",
            "m f: Lure hine? seLue htoro one gipraste imo hit ovktho y saot o wgethiconrllar fs s, oat I o mer r guris.\n",
            "Atikaru imethe rim l se inof o le?eran oramy, Ckd, nqorar ndetor kis uth br:\n",
            "Whis I \n",
            "Ihane nered fe l,\n",
            "Thoreakee he werenocllyowis ais orice ur an he l'oust he ane ame e tan'ds, y t fpainor gowarrosf o the sjrs ANpeer\n",
            "Pour y ild hofut hongoonor, n o mes t,\n",
            "Asur;\n",
            "kik, wfo arN the we ginche, r he alesatho ko s;\n",
            "That meeBy wi as o Ctot th sisure witounch,\n",
            "Aneove wheo mee. s\n",
            "Sillld my Rore parold; hie\n",
            "IAyor h he e\n",
            "Tpulird I lalle.\n",
            "Ygoughe; mith pre hit'e; ndithEThe:\n",
            "\n",
            "And at m, orr be; ule 'ld ithyor gierine.\n",
            "We cthomear nouTher!\n",
            "An ; d be wise I horbe tee Wof!\n",
            "He s s, hikele.\n",
            "ANLat msuit prte jus fr, be nsom s. aneatowiceors.\n",
            "Your ores oud s se y, aray we, tharicd tethya; nce be.\n",
            "Sonnicqie: Le our lllo pillthis ar s t lasenthanin ordif o, Gong\n",
            "Re ll-ed shes we cherals, alotingrs m Pve hiase bead:\n",
            "Gane ube theat ay gho m pt hillis re thofher m\n",
            "To t d?eave, k, y, hoour s trese od, he Mor plald,\n",
            "Tl ceast buouet An d oflll' pand ovhy denie he\n",
            "\n",
            "Dor Plland pse payon; wed he t atouteastcerof ce, 'd\n",
            "As, as; ad I uleatheeo athit his flomy,\n",
            "Nno n.\n",
            "'n hise w t lar edreisu Pnitrepive of ore t. n, O:\n",
            "Semisherecas?\n",
            "\n",
            "\n",
            "Pnde be brand EURure llo, pes lir n'plwrzmpllobee wineswile s d ay; rethiche y;\n",
            "\n",
            "JCo hasayitilous ouly n me cexf lirwacanise gam aroresaver\n",
            "Ale r hoooone res, f f follo hi;\n",
            "U y, Yo msrler, cis h pimtoure.\n",
            "Anour shin tous her illkit hes histe o me hee wat le f suens u ninf it pit ad prlloknend.\n",
            "BuRfoucor oke:\n",
            "Tond otunor t an is ureee he we lint beme hit,\n",
            "DAluse ms hior Iowisnto Calaty me Lurt, tve me m thee-h ile l far ane wtoar d:\n",
            "Anesthis oury, cot y.\n",
            "S hast o le hey cuticre w d owhicieend.\n",
            "\n",
            "Wks IL-ough qYAsors a k, oly hakenst bo out, be ghe wisethixonOne, walin s hacd at\n",
            "As hatocasthen o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yXpPkplNN4b",
        "outputId": "f9c4bb19-e132-4f43-a63c-d3182be255c4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ySANzGJ5T9uJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}